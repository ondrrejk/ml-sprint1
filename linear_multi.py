import numpy as np

# utility: split, scaling, metrics


def train_test_split(X, y, test_size=0.2, seed=42):
    rng = np.random.default_rng(
        seed
    )  # VytvoÅ™Ã­ novÃ½ generÃ¡tor nÃ¡hodnÃ½ch ÄÃ­sel (novÄ›jÅ¡Ã­ API neÅ¾ globÃ¡lnÃ­ np.random). DÃ­ky seedu mÃ­chÃ¡ vÅ¾dy stejnÄ›.
    n = len(
        X
    )  # PoÄet vzorkÅ¯ (Å™Ã¡dkÅ¯) v X. OÄekÃ¡vÃ¡me, Å¾e X.shape = (n, d(radky, sloupce)).
    idx = np.arange(n)  # Vektor indexÅ¯ [0, 1, 2, ..., n-1].
    rng.shuffle(idx)  # NÃ¡hodnÄ› promÃ­chÃ¡ indexy in-place (bez kopie).
    n_test = int(n * test_size)  # Kolik vzorkÅ¯ pÅ¯jde do testu.
    test_idx = idx[:n_test]  # RozdÄ›lÃ­ promÃ­chanÃ© indexy na test.
    train_idx = idx[n_test:]  # RozdÄ›lÃ­ promÃ­chanÃ© indexy na trÃ©nink.
    return (
        X[train_idx],
        X[test_idx],
        y[train_idx],
        y[test_idx],
    )  # VrÃ¡ti X_train, X_test, y_train, y_test pÅ™es pokroÄilÃ© indexovÃ¡nÃ­ NumPy.


def standardize(X, eps=1e-8):  # Standardizace featur: Z-score po sloupcÃ­ch.
    mean = X.mean(axis=0)  # PrÅ¯mÄ›r kaÅ¾dÃ©ho sloupce X. VÃ½sledek tvaru (d,).
    std = X.std(axis=0)  # SmÄ›rodatnÃ¡ odchylka kaÅ¾dÃ©ho sloupce. Tvar (d,).
    std = np.where(
        std < eps, 1.0, std
    )  # Ochrana proti dÄ›lenÃ­ nulou: pÅ™Ã­liÅ¡ malÃ© std nahradÃ­me 1.0, zbytek zustane std.
    Xs = (
        (X - mean) / std
    )  # Broadcasting: odeÄte sloupcovÃ© prÅ¯mÄ›ry a vydÄ›lÃ­ sloupcovÃ½m std. Xs mÃ¡ stejnÃ½ tvar jako X.
    return (
        Xs,
        mean,
        std,
    )  # VrÃ¡tÃ­ Å¡kÃ¡lovanÃ¡ data a parametry, abychom jimi Å¡kÃ¡lovali i test.


def apply_standardize(X, mean, std):  # Aplikace stejnÃ©ho Å¡kÃ¡lovÃ¡nÃ­ na novÃ¡ data.
    std = np.where(std == 0, 1.0, std)  # JeÅ¡tÄ› jednou ochrana proti nule.
    return (X - mean) / std  # StejnÃ© Z-score s danÃ½mi mean a std.


def mse(y_true, y_pred):
    return np.mean(
        (y_true - y_pred) ** 2
    )  # Mean squared error. y_true a y_pred majÃ­ tvar (n,)


def rmse(y_true, y_pred):
    return np.sqrt(mse(y_true, y_pred))  # Odmocnina MSE, zpÃ¡tky v jednotkÃ¡ch cÃ­le.


def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))  # PrÅ¯mÄ›r absolutnÃ­ch hodnot chyb.


def r2(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    return (
        1 - ss_res / ss_tot
    )  # Koeficient determinace. ss_res je zbytkovÃ¡ suma ÄtvercÅ¯, ss_tot celkovÃ¡. 1 je ideÃ¡l, 0 je â€stejnÃ© jako prÅ¯mÄ›râ€œ.


# Model
def predict(X, w, b):
    return X @ w + b


# Predikce lineÃ¡rnÃ­ho modelu:

#   X: (n, d), w: (d,), vÃ½sledek X @ w: (n,).
#   +b se broadcastne na kaÅ¾dÃ½ prvek.
#   Pro dot product je dÅ¯leÅ¾itÃ© pouÅ¾Ã­t operÃ¡tor @ namÃ­sto *.


# TrÃ©nink: Gradient descent
def fit_linear_gd(
    X,
    y,
    lr=0.1,
    epochs=1000,
    l2=0.0,  # Ridge koeficient (lambda)
    l1=0.0,  # Lasso koeficient (lambda)
    verbose_every=100,
):  # HlavnÃ­ trenÃ©r. lr je krok uÄenÃ­, epochs poÄet prÅ¯chodÅ¯, l2/l1 penalizace vah, verbose_every frekvence logu.
    # Gradient descent pro multivariÃ¡tnÃ­ lineÃ¡rnÃ­ regresi s volitelnou L2 a L1.
    # Regularizace se aplikuje jen na w (ne na bias b).
    n, d = X.shape  # PoÄet vzorkÅ¯ a poÄet featur.
    w = np.zeros(d, dtype=float)
    b = 0.0
    # Inicializace parametrÅ¯ na nulu. w tvar (d,), b skalar.
    for epoch in range(1, epochs + 1):
        # HlavnÃ­ trÃ©ninkovÃ¡ smyÄka. 1 aÅ¾ epochs vÄetnÄ›.
        y_pred = predict(X, w, b)  # DopÅ™ednÃ½ prÅ¯chod: y_pred tvar (n,).
        error = y - y_pred  # Residua. KlidnÄ› by Å¡lo residuals.
        dw = -(2.0 / n) * (X.T @ error)
        # X.T: (d, n), error: (n,), takÅ¾e X.T @ error: (d,).
        # Faktor -(2/n) vyplÃ½vÃ¡ z derivace weights podle y_pred = Xw+b
        db = -(2.0 / n) * np.sum(
            error
        )  # Gradient MSE vÅ¯Äi biasu b. Suma pÅ™es vÅ¡echna residua, krÃ¡t -(2/n).
        if l2 > 0:
            dw += 2.0 * l2 * w
            # Ridge penalizace
        if l1 > 0:
            dw += l1 * np.sign(w)
            # Lasso penalizace
        w -= lr * dw
        b -= lr * db
        # Update parametrÅ¯ po smÄ›ru zÃ¡pornÃ©ho gradientu. KdyÅ¾ ti to diverguje, lr je moc velikÃ©. KdyÅ¾ to leze jak Å¡nek, je moc malÃ©.
        if verbose_every and epoch % verbose_every == 0:
            loss = mse(y, y_pred)
            print(f"epoch {epoch:4d}  MSE={loss:.6f}")
            # LogovÃ¡nÃ­ prÅ¯bÄ›hu. PomÃ¡hÃ¡ chytat â€jojo efektâ€œ gradientu a dalÅ¡Ã­ katastrofy.
        return w, b  # VrÃ¡tÃ­ nauÄenÃ© koeficienty.


# Demo (spouÅ¡tÃ­ se jen pÅ™i python linear_multi.py)

if (
    __name__ == "__main__"
):  # ZajistÃ­, Å¾e tenhle blok bÄ›Å¾Ã­ jen kdyÅ¾ soubor spouÅ¡tÃ­Å¡ pÅ™Ã­mo, ne pÅ™i importu.
    np.random.seed(
        2
    )  # NastavÃ­ globÃ¡lnÃ­ RNG v np.random (starÅ¡Ã­ API) pro opakovatelnost v ÄÃ¡sti, kde ho pouÅ¾Ã­vÃ¡me (rand, randn). Ano, vÃ½Å¡e jsme pouÅ¾ili novÄ›jÅ¡Ã­ default_rng v train_test_split; obÄ› cesty jsou platnÃ©.
    n, d = 200, 3  # PoÄet vzorkÅ¯ a featur v syntetickÃ½ch datech.
    X = np.random.rand(
        n, d
    )  # Matice X tvaru (200, 3), hodnoty z uniformnÃ­ho rozdÄ›lenÃ­ na intervalu [0, 1).
    true_w = np.array(
        [3.0, -2.0, 5.0]
    )  # SkuteÄnÃ© vÃ¡hy, kterÃ½mi generujeme cÃ­lovÃ© y. Tvar (3,).
    true_b = 4.0  # SkuteÄnÃ½ bias
    noise = np.random.randn(n) * 0.5  # GaussovskÃ½ Å¡um
    # Vektor Å¡umu tvaru (200,) ze standardnÃ­ normÃ¡lnÃ­ N(0,1), Å¡kÃ¡lovanÃ½ na smÄ›rodatnou odchylku 0.5.
    y = (
        X @ true_w + true_b + noise
    )  # VytvoÅ™Ã­ cÃ­lovÃ© hodnoty: perfektnÃ­ lineÃ¡rnÃ­ model plus Å¡um.
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, seed=7
    )  # RozdÄ›lÃ­ na 75 % trÃ©nink, 25 % test. JinÃ½ seed, aÅ¥ vidÃ­Å¡, Å¾e split a generovÃ¡nÃ­ dat jsou nezÃ¡vislÃ©.
    X_train_s, mean, std = standardize(
        X_train
    )  # Standardizace trÃ©ninku: vrÃ¡tÃ­ Å¡kÃ¡lovanÃ¡ data a statistiky po sloupcÃ­ch.
    X_test_s = apply_standardize(
        X_test, mean, std
    )  # Aplikuje stejnÃ© Å¡kÃ¡lovÃ¡nÃ­ na test (nepoÄÃ­tÃ¡me mean/std z testu, Å¾Ã¡dnÃ½ leakage).
    print("=== Bez regularizace ===")  # DekorativnÃ­ pruh. Abys vÄ›dÄ›l, co se prÃ¡vÄ› dÄ›je.
    w, b = fit_linear_gd(X_train_s, y_train, lr=0.1, epochs=1000, verbose_every=200)
    # TrÃ©nuje ÄistÃ½ MSE model:
    #   X_train_s: Å¡kÃ¡lovanÃ© featury,
    #   lr=0.1, epochs=1000,
    #   bez L1/L2.
    y_pred_train = predict(X_train_s, w, b)  # Predikce na trÃ©ninku.
    y_pred_test = predict(X_test_s, w, b)  # Predikce na testu.
    print("\nTrÃ©nink:")  # Jen text na pÅ™ehlednost.
    print(
        f"  MSE={mse(y_train, y_pred_train):.4f}  RMSE={rmse(y_train, y_pred_train):.4f}  MAE={mae(y_train, y_pred_train):.4f}  R2={r2(y_train, y_pred_train):.4f}"
    )  # Vytiskne ÄtyÅ™i metriky na trÃ©ninku.
    print("Test:")  # HlaviÄka pro test.
    print(
        f"  MSE={mse(y_test, y_pred_test):.4f}  RMSE={rmse(y_test, y_pred_test):.4f}  MAE={mae(y_test, y_pred_test):.4f}  R2={r2(y_test, y_pred_test):.4f}"
    )  # A totÃ©Å¾ pro test. Pokud je R2 na testu vÃ½raznÄ› horÅ¡Ã­ neÅ¾ na train, pÅ™euÄujeÅ¡. Pokud jsou oba bÃ­dnÃ©, model je poddimenzovanÃ½ nebo data jsou chaos.
    print(
        f"VÃ¡hy w (na Å¡kÃ¡lovanÃ½ch featurÃ¡ch): {w}"
    )  # Vytiskne nauÄenÃ© vÃ¡hy. Pozor: tohle jsou vÃ¡hy ve Å¡kÃ¡lovanÃ©m prostoru. Nejsou pÅ™Ã­mo srovnatelnÃ© s true_w, protoÅ¾e jsme standardizovali X.
    print(f"Bias b: {b:.4f}")  # Vytiskne bias (opÄ›t ve Å¡kÃ¡lovanÃ©m prostoru).
    print("\n=== Ridge (L2=0.1) ===")  # ZaÄÃ¡tek trÃ©ninku s L2.
    w_r, b_r = fit_linear_gd(
        X_train_s, y_train, lr=0.1, epochs=1000, l2=0.1, verbose_every=200
    )  # TrÃ©nink s Ridge penalizacÃ­. StejnÃ½ lr, epochs, jen l2=0.1.
    y_pred_test_r = predict(X_test_s, w_r, b_r)  # Predikce Ridge modelu na testu.
    print(
        f"Test R2: {r2(y_test, y_pred_test_r):.4f}  | w={w_r}, b={b_r:.4f}"
    )  # RychlÃ© shrnutÃ­ vÃ½konu a koeficientÅ¯.
    print("\n=== Lasso (L1=0.05) ===")  # ZaÄÃ¡tek trÃ©ninku s L1.
    w_l, b_l = fit_linear_gd(
        X_train_s, y_train, lr=0.05, epochs=1200, l1=0.05, verbose_every=300
    )  # Lasso trÃ©nink:
    #   menÅ¡Ã­ lr (L1 bÃ½vÃ¡ ostÅ™ejÅ¡Ã­),
    #   vÃ­c epoch, aby to dobÄ›hlo do rozumnÃ© konvergence,
    #   l1=0.05 penalizace.
    y_pred_test_l = predict(X_test_s, w_l, b_l)  # Predikce Lasso modelu na testu.
    print(
        f"Test R2: {r2(y_test, y_pred_test_l):.4f}  | w={w_l}, b={b_l:.4f}"
    )  # VÃ½sledky Lasso. U Lassa Äasto uvidÃ­Å¡ nÄ›kterÃ© vÃ¡hy pÅ™esnÄ› nula, pokud feature nic nepÅ™inÃ¡Å¡Ã­.

# DrobnÃ©, ale dÅ¯leÅ¾itÃ© detaily navÃ­c
# VÃ½poÄetnÃ­ sloÅ¾itost jednÃ© epochy je ~ ğ‘‚(ğ‘›ğ‘‘) (matice-vektor vÄ›ci).

#   Standardizace a vÃ¡hy: pokud chceÅ¡ koeficienty v pÅ¯vodnÃ­ Å¡kÃ¡le, pÅ™eveÄ je zpÄ›t (viz poznÃ¡mka u tisku vah).

#   Konvergence: kdyÅ¾ MSE neklesÃ¡, sniÅ¾ lr; kdyÅ¾ osciluje, taky sniÅ¾; kdyÅ¾ klesÃ¡ moc pomalu, zvÃ½Å¡.

#   Regularizace:

#       L2 â€zmenÅ¡ujeâ€œ vÅ¡echny vÃ¡hy hladce.

#       L1 tlaÄÃ­ malÃ© vÃ¡hy k nule, ÄÃ­mÅ¾ dÄ›lÃ¡ implicitnÃ­ vÃ½bÄ›r featur.

#   Bias nikdy neregularizuj. NenÃ­ to â€sloÅ¾itostâ€œ modelu, je to posun.
